{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3uernfiXfzU",
        "outputId": "43a48fd6-020b-4987-8642-96555a345e9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-26 15:57:07.363242: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-05-26 15:57:07.387395: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-05-26 15:57:07.777448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRC: cloth3d++_subset/\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-26 15:57:09.716749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.732755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.732875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.734704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.734959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.735014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.765646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.765731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.765785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-05-26 15:57:09.765837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6264 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of data used to train:\n",
            "119856\n",
            "Input shape: (1, 256, 256, 3)\n",
            "Label shape: (1, 256, 256)\n",
            "Learning rate: 1e-05\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-05-26 15:57:24.309402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
            "2024-05-26 15:57:24.366453: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:231] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.9\n",
            "2024-05-26 15:57:24.366463: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Used ptxas at ptxas\n",
            "2024-05-26 15:57:24.366485: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
            "Relying on driver to perform ptx compilation. \n",
            "Modify $PATH to customize ptxas location.\n",
            "This message will be only logged once.\n",
            "2024-05-26 15:57:24.801227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
            "2024-05-26 15:57:25.343958: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x58e7b5ce0c30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-05-26 15:57:25.343976: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Laptop GPU, Compute Capability 8.9\n",
            "2024-05-26 15:57:25.346436: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "2024-05-26 15:57:25.384176: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    122/Unknown - 34s 82ms/step - loss: 0.0331 - mae: 0.1034"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 282\u001b[0m\n\u001b[1;32m    279\u001b[0m mc \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m, experiment_name, checkpoint), monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mae\u001b[39m\u001b[38;5;124m'\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# training the model and saving the history (.take(10))\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtensorboard_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensorboard_lr_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmc\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# Plot and write the history\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(experiment_name):\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    146\u001b[0m   (concrete_function,\n\u001b[1;32m    147\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1348\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1349\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1350\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1351\u001b[0m     args,\n\u001b[1;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1353\u001b[0m     executing_eagerly)\n\u001b[1;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    195\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 196\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mlist\u001b[39m(args))\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1455\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1458\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1460\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1461\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1462\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1465\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1466\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1467\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1471\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1472\u001b[0m   )\n",
            "File \u001b[0;32m~/anaconda3/envs/tf3dcloth/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# %%\n",
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import binary_dilation, binary_erosion\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_unet_collection import models\n",
        "from tensorflow import keras\n",
        "from keras_unet_collection.utils import dummy_loader\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import binary_dilation, binary_erosion\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Path to data\n",
        "SRC = 'cloth3d++_subset/'\n",
        "print('SRC:', SRC)\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 4\n",
        "\n",
        "# %%\n",
        "class TFRecordDataHandler:\n",
        "    def __init__(self, tfrecord_file, batch_size=32, shuffle=True, augment=False):\n",
        "        self.tfrecord_file = tfrecord_file\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "\n",
        "    def _parse_function(self, proto):\n",
        "        feature_description = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string),\n",
        "            'depth': tf.io.FixedLenFeature([], tf.string),\n",
        "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth_width': tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "        parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "\n",
        "        image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
        "        depth = tf.io.decode_raw(parsed_features['depth'], tf.float32)\n",
        "\n",
        "        height = parsed_features['height']\n",
        "        width = parsed_features['width']\n",
        "\n",
        "        image = tf.reshape(image, [height, width, 3])\n",
        "        depth = tf.reshape(depth, [parsed_features['depth_height'], parsed_features['depth_width']])\n",
        "\n",
        "        return image, depth\n",
        "\n",
        "    def _normalize(self, image, depth):\n",
        "        # Convert image to float for processing and normalize to range [0, 1]\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "        # Create a mask where depth values are greater than zero\n",
        "        depth_mask = depth > 0\n",
        "\n",
        "        # Normalize depth based on masked regions\n",
        "        # Calculate the mean of the depth where it is greater than zero\n",
        "        depth_values = tf.boolean_mask(depth, depth_mask)\n",
        "        depth_mean = tf.reduce_mean(depth_values)\n",
        "\n",
        "        # Subtract the mean from the depth values where mask is true\n",
        "        depth = tf.where(depth_mask, depth - depth_mean, depth)\n",
        "\n",
        "        # Set depth values less than 0 to 0 after subtraction\n",
        "        depth = tf.maximum(depth, 0)\n",
        "\n",
        "        # Prepare the mask for RGB image normalization\n",
        "        mask = tf.tile(tf.expand_dims(depth_mask, axis=-1), [1, 1, 3])\n",
        "\n",
        "        # Masked image for mean and std deviation calculation\n",
        "        masked_image = tf.boolean_mask(image, mask)\n",
        "        mean, variance = tf.nn.moments(masked_image, axes=[0])\n",
        "        std_dev = tf.sqrt(variance + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "        # Apply the mask to image normalization\n",
        "        normalized_image = tf.where(\n",
        "            mask,\n",
        "            (image - mean) / std_dev,\n",
        "            image  # Preserve original pixels where mask is False\n",
        "        )\n",
        "\n",
        "        return normalized_image, depth\n",
        "\n",
        "    def _augment(self, image, depth):\n",
        "        if tf.random.uniform(()) > 0.5:\n",
        "            image = tf.image.flip_left_right(image)\n",
        "            depth = tf.image.flip_left_right(tf.expand_dims(depth, axis=-1))\n",
        "            depth = tf.squeeze(depth, axis=-1)\n",
        "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "        return image, depth\n",
        "\n",
        "    def load_dataset(self):\n",
        "        dataset = tf.data.TFRecordDataset(self.tfrecord_file)\n",
        "        dataset = dataset.map(self._parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(self._normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        if self.augment:\n",
        "            dataset = dataset.map(self._augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        if self.shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n",
        "    \n",
        "    def get_num_samples(self):\n",
        "        dataset = tf.data.TFRecordDataset(self.tfrecord_file)\n",
        "        count = 0\n",
        "        for _ in dataset:\n",
        "            count += 1\n",
        "        return count\n",
        "\n",
        "\n",
        "# %%\n",
        "\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer.lr\n",
        "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            lr = lr(self.model.optimizer.iterations)\n",
        "        print(f\"Epoch {epoch + 1}: Learning rate is {tf.keras.backend.get_value(lr):.6f}\")\n",
        "\n",
        "class TensorBoardLearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, log_dir='./logs'):\n",
        "        super(TensorBoardLearningRateLogger, self).__init__()\n",
        "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer.lr\n",
        "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            lr = lr(self.model.optimizer.iterations)\n",
        "        with self.file_writer.as_default():\n",
        "            tf.summary.scalar('learning_rate', tf.keras.backend.get_value(lr), step=epoch)\n",
        "\n",
        "# %%\n",
        "def visualize_hist(history, show=True, filename=None, title='Training history'):\n",
        "    train_hist = history.history\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax1.plot(train_hist['loss'])\n",
        "    ax1.plot(train_hist['val_loss'])\n",
        "    ax1.set(xlabel='epoch', ylabel='Loss')\n",
        "    ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "    ax2.plot(train_hist['mae'])\n",
        "    ax2.plot(train_hist['val_mae'])\n",
        "    ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "    ax2.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    if filename is not None:\n",
        "        fig.savefig(filename)\n",
        "\n",
        "\n",
        "# %%\n",
        "class WarmUpCosineDecayScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_learning_rate, target_learning_rate, total_steps, warmup_steps):\n",
        "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.target_learning_rate = target_learning_rate\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.decay_steps = total_steps - warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Convert to float32 to ensure the operations are compatible\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        decay_steps = tf.cast(self.decay_steps, tf.float32)\n",
        "\n",
        "        # Compute the warmup learning rate\n",
        "        warmup_lr = self.initial_learning_rate + (self.target_learning_rate - self.initial_learning_rate) * (step / warmup_steps)\n",
        "\n",
        "        # Compute the cosine decay learning rate\n",
        "        cosine_decay = 0.5 * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
        "        decayed_lr = (self.target_learning_rate - self.initial_learning_rate) * cosine_decay + self.initial_learning_rate\n",
        "\n",
        "        # Choose the learning rate based on the step\n",
        "        learning_rate = tf.where(step < warmup_steps, warmup_lr, decayed_lr)\n",
        "        return learning_rate\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'initial_learning_rate': self.initial_learning_rate,\n",
        "            'target_learning_rate': self.target_learning_rate,\n",
        "            'total_steps': self.total_steps,\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }\n",
        "\n",
        "# %%\n",
        "# Parameters\n",
        "initial_learning_rate = 1e-5\n",
        "target_learning_rate = 1e-2\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "checkpoint = './best_model.h5'\n",
        "\n",
        "trainloader = TFRecordDataHandler('train.tfrecords', batch_size=1, shuffle=shuffle, augment=True)\n",
        "valloader = TFRecordDataHandler('validation.tfrecords', batch_size=1, shuffle=shuffle, augment=False)\n",
        "train_dg = trainloader.load_dataset()\n",
        "validation_dg = valloader.load_dataset()\n",
        "\n",
        "num_samples_train = trainloader.get_num_samples()\n",
        "\n",
        "print('Number of data used to train:')\n",
        "print(num_samples_train * batch_size)\n",
        "# print('')\n",
        "\n",
        "for x, y in train_dg.take(1):\n",
        "    print(\"Input shape:\", x.shape)  # Should be (batch_size, height, width, channels)\n",
        "    print(\"Label shape:\", y.shape)  # Should be (batch_size, ...) depending on your task\n",
        "\n",
        "\n",
        "total_steps = num_samples_train // num_samples_train * n_epochs\n",
        "\n",
        "\n",
        "import datetime\n",
        "for wu_ratio in [0.1]:\n",
        "\n",
        "    warmup_steps = int(wu_ratio * total_steps)\n",
        "\n",
        "    folder_name = f'transunet'\n",
        "     #get the time of the experiment\n",
        "    time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    experiment_name = folder_name + '_' + time\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(\"logs\",  experiment_name))\n",
        "    tensorboard_lr_logger = TensorBoardLearningRateLogger(os.path.join(\"logs\", experiment_name))\n",
        "\n",
        "\n",
        "    # Create the learning rate schedule\n",
        "    lr_schedule = WarmUpCosineDecayScheduler(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        target_learning_rate=target_learning_rate,\n",
        "        total_steps=total_steps,\n",
        "        warmup_steps=warmup_steps\n",
        "    )\n",
        "\n",
        "    # Define the optimizer with the custom learning rate schedule\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    model = models.transunet_2d((256, 256, 3), [64, 128, 256, 512, 1024], n_labels=1,\n",
        "                                stack_num_down=2, stack_num_up=1, \n",
        "                                activation='GELU', output_activation='Sigmoid',\n",
        "                                batch_norm=True, pool='max', unpool=False, name='transunet_2d', embed_dim=100, num_heads=4)\n",
        "\n",
        "    print('Learning rate:', initial_learning_rate)\n",
        "    # defining the optimizer\n",
        "    model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
        "\n",
        "    # es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=3)\n",
        "\n",
        "    # saving the best model based on val_loss\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(os.path.join(\"models\", experiment_name, checkpoint), monitor='val_mae', mode='min', save_best_only=True)\n",
        "    \n",
        "    # training the model and saving the history (.take(10))\n",
        "    history = model.fit(train_dg, validation_data=validation_dg, epochs=n_epochs, verbose=verbose, workers=-1, use_multiprocessing=True,\n",
        "                        callbacks=[tensorboard_callback, tensorboard_lr_logger, mc])\n",
        "\n",
        "    # Plot and write the history\n",
        "    if not os.path.exists(experiment_name):\n",
        "        os.makedirs(experiment_name)\n",
        "    filename = os.path.join(experiment_name, 'train_history.jpg')\n",
        "    visualize_hist(history, show=True, filename=filename)\n",
        "\n",
        "    #with open('train_history_lr_'+str(lr)+'.pkl', 'wb') as handle:\n",
        "    #    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    # Get the best validation loss and the epoch\n",
        "\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss)\n",
        "    # d_lr[lr] = (best_val_loss, best_epoch)\n",
        "    print('Best validation loss:', best_val_loss, 'at epoch', best_epoch)\n",
        "\n",
        "    # Clear gpu memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "# %%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
