{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3uernfiXfzU",
        "outputId": "43a48fd6-020b-4987-8642-96555a345e9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SRC: cloth3d++_subset/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import binary_dilation, binary_erosion\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_unet_collection import models\n",
        "from tensorflow import keras\n",
        "from keras_unet_collection.utils import dummy_loader\n",
        "import cv2\n",
        "import gc\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import binary_dilation, binary_erosion\n",
        "from skimage.transform import resize\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Path to data\n",
        "SRC = 'cloth3d++_subset/'\n",
        "print('SRC:', SRC)\n",
        "\n",
        "N_TRAIN = 128\n",
        "N_VAL = 16\n",
        "\n",
        "n_epochs = 20\n",
        "batch_size = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "mvtrlRyHPhEO"
      },
      "outputs": [],
      "source": [
        "class TFRecordDataHandler:\n",
        "    def __init__(self, tfrecord_file, batch_size=32, shuffle=True, augment=False):\n",
        "        self.tfrecord_file = tfrecord_file\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.augment = augment\n",
        "\n",
        "    def _parse_function(self, proto):\n",
        "        feature_description = {\n",
        "            'image': tf.io.FixedLenFeature([], tf.string),\n",
        "            'depth': tf.io.FixedLenFeature([], tf.string),\n",
        "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth_height': tf.io.FixedLenFeature([], tf.int64),\n",
        "            'depth_width': tf.io.FixedLenFeature([], tf.int64)\n",
        "        }\n",
        "        parsed_features = tf.io.parse_single_example(proto, feature_description)\n",
        "\n",
        "        image = tf.io.decode_raw(parsed_features['image'], tf.uint8)\n",
        "        depth = tf.io.decode_raw(parsed_features['depth'], tf.float32)\n",
        "\n",
        "        height = parsed_features['height']\n",
        "        width = parsed_features['width']\n",
        "\n",
        "        image = tf.reshape(image, [height, width, 3])\n",
        "        depth = tf.reshape(depth, [parsed_features['depth_height'], parsed_features['depth_width']])\n",
        "\n",
        "        return image, depth\n",
        "\n",
        "    def _normalize(self, image, depth):\n",
        "        # Convert image to float for processing and normalize to range [0, 1]\n",
        "        image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "        # Create a mask where depth values are greater than zero\n",
        "        depth_mask = depth > 0\n",
        "\n",
        "        # Normalize depth based on masked regions\n",
        "        # Calculate the mean of the depth where it is greater than zero\n",
        "        depth_values = tf.boolean_mask(depth, depth_mask)\n",
        "        depth_mean = tf.reduce_mean(depth_values)\n",
        "\n",
        "        # Subtract the mean from the depth values where mask is true\n",
        "        depth = tf.where(depth_mask, depth - depth_mean, depth)\n",
        "\n",
        "        # Set depth values less than 0 to 0 after subtraction\n",
        "        depth = tf.maximum(depth, 0)\n",
        "\n",
        "        # Prepare the mask for RGB image normalization\n",
        "        mask = tf.tile(tf.expand_dims(depth_mask, axis=-1), [1, 1, 3])\n",
        "\n",
        "        # Masked image for mean and std deviation calculation\n",
        "        masked_image = tf.boolean_mask(image, mask)\n",
        "        mean, variance = tf.nn.moments(masked_image, axes=[0])\n",
        "        std_dev = tf.sqrt(variance + 1e-6)  # Adding epsilon to avoid division by zero\n",
        "\n",
        "        # Apply the mask to image normalization\n",
        "        normalized_image = tf.where(\n",
        "            mask,\n",
        "            (image - mean) / std_dev,\n",
        "            image  # Preserve original pixels where mask is False\n",
        "        )\n",
        "\n",
        "        return normalized_image, depth\n",
        "\n",
        "    def _augment(self, image, depth):\n",
        "        if tf.random.uniform(()) > 0.5:\n",
        "            image = tf.image.flip_left_right(image)\n",
        "            depth = tf.image.flip_left_right(tf.expand_dims(depth, axis=-1))\n",
        "            depth = tf.squeeze(depth, axis=-1)\n",
        "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "        return image, depth\n",
        "\n",
        "    def load_dataset(self):\n",
        "        dataset = tf.data.TFRecordDataset(self.tfrecord_file)\n",
        "        dataset = dataset.map(self._parse_function, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        dataset = dataset.map(self._normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        if self.augment:\n",
        "            dataset = dataset.map(self._augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        if self.shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=1000)\n",
        "\n",
        "        dataset = dataset.batch(self.batch_size)\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VqMXIYooQ42i"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer.lr\n",
        "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            lr = lr(self.model.optimizer.iterations)\n",
        "        print(f\"Epoch {epoch + 1}: Learning rate is {tf.keras.backend.get_value(lr):.6f}\")\n",
        "\n",
        "class TensorBoardLearningRateLogger(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, log_dir='./logs'):\n",
        "        super(TensorBoardLearningRateLogger, self).__init__()\n",
        "        self.file_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        lr = self.model.optimizer.lr\n",
        "        if isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "            lr = lr(self.model.optimizer.iterations)\n",
        "        with self.file_writer.as_default():\n",
        "            tf.summary.scalar('learning_rate', tf.keras.backend.get_value(lr), step=epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YAZZ0Q1aWPnt"
      },
      "outputs": [],
      "source": [
        "def visualize_hist(history, show=True, filename=None, title='Training history'):\n",
        "    train_hist = history.history\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 4))\n",
        "    fig.suptitle(title, fontsize=14, fontweight='bold')\n",
        "\n",
        "    ax1.plot(train_hist['loss'])\n",
        "    ax1.plot(train_hist['val_loss'])\n",
        "    ax1.set(xlabel='epoch', ylabel='Loss')\n",
        "    ax1.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "    ax2.plot(train_hist['mae'])\n",
        "    ax2.plot(train_hist['val_mae'])\n",
        "    ax2.set(xlabel='epoch', ylabel='MAE')\n",
        "    ax2.legend(['train', 'valid'], loc='upper right')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    if filename is not None:\n",
        "        fig.savefig(filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Lv-hK_godkrl"
      },
      "outputs": [],
      "source": [
        "class WarmUpCosineDecayScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, initial_learning_rate, target_learning_rate, total_steps, warmup_steps):\n",
        "        super(WarmUpCosineDecayScheduler, self).__init__()\n",
        "        self.initial_learning_rate = initial_learning_rate\n",
        "        self.target_learning_rate = target_learning_rate\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.decay_steps = total_steps - warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        # Convert to float32 to ensure the operations are compatible\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
        "        decay_steps = tf.cast(self.decay_steps, tf.float32)\n",
        "\n",
        "        # Compute the warmup learning rate\n",
        "        warmup_lr = self.initial_learning_rate + (self.target_learning_rate - self.initial_learning_rate) * (step / warmup_steps)\n",
        "\n",
        "        # Compute the cosine decay learning rate\n",
        "        cosine_decay = 0.5 * (1 + tf.cos(np.pi * (step - warmup_steps) / decay_steps))\n",
        "        decayed_lr = (self.target_learning_rate - self.initial_learning_rate) * cosine_decay + self.initial_learning_rate\n",
        "\n",
        "        # Choose the learning rate based on the step\n",
        "        learning_rate = tf.where(step < warmup_steps, warmup_lr, decayed_lr)\n",
        "        return learning_rate\n",
        "\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            'initial_learning_rate': self.initial_learning_rate,\n",
        "            'target_learning_rate': self.target_learning_rate,\n",
        "            'total_steps': self.total_steps,\n",
        "            'warmup_steps': self.warmup_steps\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uYYTcSsWQRw",
        "outputId": "8c2396d7-92c8-43bf-a9b9-79c2b6b519c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of data used to train:\n",
            "372\n",
            "\n",
            "Input shape: (2, 256, 256, 3)\n",
            "Label shape: (2, 256, 256)\n",
            "Learning rate: 1e-05\n",
            "Epoch 1/20\n",
            "186/186 [==============================] - 8799s 47s/step - loss: 0.0435 - mae: 0.1383 - val_loss: 0.0422 - val_mae: 0.1331\n",
            "Epoch 2/20\n",
            " 66/186 [=========>....................] - ETA: 1:25:33 - loss: 0.0083 - mae: 0.0467"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "initial_learning_rate = 1e-5\n",
        "target_learning_rate = 1e-2\n",
        "verbose = 1\n",
        "shuffle = True\n",
        "checkpoint = './best_model.h5'\n",
        "\n",
        "\n",
        "\n",
        "train_dg = TFRecordDataHandler('train.tfrecords', batch_size=batch_size, shuffle=shuffle, augment=True).load_dataset()\n",
        "validation_dg = TFRecordDataHandler('validation.tfrecords', batch_size=batch_size, shuffle=shuffle, augment=False).load_dataset()\n",
        "print('Number of data used to train:')\n",
        "print(len(list(train_dg))* batch_size)\n",
        "print('')\n",
        "\n",
        "for x, y in train_dg.take(1):\n",
        "    print(\"Input shape:\", x.shape)  # Should be (batch_size, height, width, channels)\n",
        "    print(\"Label shape:\", y.shape)  # Should be (batch_size, ...) depending on your task\n",
        "\n",
        "\n",
        "total_steps = len(list(train_dg)) * n_epochs\n",
        "\n",
        "\n",
        "for wu_ratio in [0.1]:\n",
        "\n",
        "    warmup_steps = int(wu_ratio * total_steps)\n",
        "\n",
        "    experiment_name = f'/lr_decay_{initial_learning_rate}_{target_learning_rate}_wu_ratio_{wu_ratio}'\n",
        "\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join(experiment_name, 'TensorBoard'))\n",
        "    tensorboard_lr_logger = TensorBoardLearningRateLogger(os.path.join(experiment_name, 'TensorBoard'))\n",
        "\n",
        "\n",
        "    # Create the learning rate schedule\n",
        "    lr_schedule = WarmUpCosineDecayScheduler(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        target_learning_rate=target_learning_rate,\n",
        "        total_steps=total_steps,\n",
        "        warmup_steps=warmup_steps\n",
        "    )\n",
        "\n",
        "    # Define the optimizer with the custom learning rate schedule\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "\n",
        "    model = models.transunet_2d((256, 256, 3), [64, 128, 256, 512, 1024], n_labels=1,\n",
        "                            stack_num_down=2, stack_num_up=1,\n",
        "                            activation='GELU', output_activation='Sigmoid',\n",
        "                            batch_norm=True, pool='max', unpool=False, name='transunet_2d')\n",
        "\n",
        "    print('Learning rate:', initial_learning_rate)\n",
        "    # defining the optimizer\n",
        "    model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=['mae'])\n",
        "\n",
        "    # es = tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=3)\n",
        "\n",
        "    # saving the best model based on val_loss\n",
        "    # mc = tf.keras.callbacks.ModelCheckpoint(checkpoint, monitor='val_mae', mode='min', save_best_only=True, save_freq=5)\n",
        "\n",
        "    # training the model and saving the history\n",
        "    history = model.fit(train_dg, validation_data=validation_dg, epochs=n_epochs, verbose=verbose, workers=2, use_multiprocessing=True,\n",
        "\n",
        "                        )\n",
        "\n",
        "    # Plot and write the history\n",
        "    if not os.path.exists(experiment_name):\n",
        "        os.makedirs(experiment_name)\n",
        "    filename = os.path.join(experiment_name, 'train_history.jpg')\n",
        "    visualize_hist(history, show=True, filename=filename)\n",
        "\n",
        "    #with open('train_history_lr_'+str(lr)+'.pkl', 'wb') as handle:\n",
        "    #    pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    # Get the best validation loss and the epoch\n",
        "\n",
        "    best_val_loss = min(history.history['val_loss'])\n",
        "    best_epoch = history.history['val_loss'].index(best_val_loss)\n",
        "    # d_lr[lr] = (best_val_loss, best_epoch)\n",
        "    print('Best validation loss:', best_val_loss, 'at epoch', best_epoch)\n",
        "\n",
        "    # Clear gpu memory\n",
        "    del model\n",
        "    gc.collect()\n",
        "    tf.keras.backend.clear_session()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
